{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8168211-1206-42be-9ee4-01db25969aba",
   "metadata": {},
   "source": [
    "# Cab Fare Estimation\n",
    "\n",
    "Using machine learning and analytics to build a regression model. This model will predict the cost of a taxi trip using several numeric features of the ride. The model was built using a dataset of Chicago taxi trips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f085c2-9d07-4f3c-a7f3-489dddc4918f",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "\n",
    "To train and test a supervised learning model that can predict the cost of a taxi ride using several features of the ride. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9429b-ea0f-4013-b620-4f9534455a93",
   "metadata": {},
   "source": [
    "## Dataset: Taxi Trips Chicago 2024\n",
    "\n",
    "__Dataset:__ https://www.kaggle.com/datasets/adelanseur/taxi-trips-chicago-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9d82a-26ab-432d-b7f4-d39fccf50b65",
   "metadata": {},
   "source": [
    "__Data Description__\n",
    "\n",
    "The initial dataset consisted of 23 columns of mostly numeric features. Each row was an individual trip with the following attributes:\n",
    "\n",
    "- Trip ID: a unique identifier for each trip\n",
    "- Taxi ID: identifier for the taxi cab\n",
    "- Trip Start Timestamp: time trip started\n",
    "- Trip End Timestamp: time trip ended\n",
    "- Trip Seconds: time in seconds\n",
    "- Trip Miles: distance in miles\n",
    "- Pickup Census Tract: Census Tract where trip began\n",
    "- Dropoff Census Tract: Census Tract where trip ended\n",
    "- Pickup Community Area: Community Area where the trip began\n",
    "- Dropoff Community Area: Community Area where the trip ended\n",
    "- Fare: fare for the trip\n",
    "- Tips: tip given for the trip\n",
    "- Tolls: any tolls accrued during trip\n",
    "- Extras: any extra charges for the trip\n",
    "- Trip Total: sum of fare, tolls, tips, extra charges\n",
    "- Payment Type: payment type used\n",
    "- Company: taxi company\n",
    "- Pickup Centroid Latitude: latitude of the center of the pickup census tract or the community area\n",
    "- Pickup Centroid Longitude: longitude of the center of the pickup census tract or the community area\n",
    "- Pickup Centroid Location: location of the center of the pickup census tract or the community area\n",
    "- Dropoff Centroid Latitude: latitude of the center of the dropoff census tract or the community area\n",
    "- Dropoff Centroid Longitude: longitude of the center of the dropoff census tract or the community area\n",
    "- Dropoff Centroid Location: location of the center of the dropoff census tract or the community area \n",
    "\n",
    "There were 865,247 rows in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706c86b-fdc9-42ae-b30d-b46be85bc8bc",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810f207-4740-453c-a6ea-535399dabd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fdd9c-9c8a-4f78-8150-e21f0d7281d5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33940364-4ed9-4963-aab3-059765cc19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load initial csv\n",
    "initial_data = pd.read_csv('Taxi_Trips.csv')\n",
    "\n",
    "#show columns in initial dataset\n",
    "print('Columns in initial dataframe:')\n",
    "print(initial_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34acdd7-bab4-4fc6-878b-4ccaba4efb99",
   "metadata": {},
   "source": [
    "I did not want to keep all of the columns in the initial dataset. I created a new dataframe by selecting only features I thought would be most impactful on trip cost. Also, there were a few columns that contained pickup and dropoff location information and I did not need all of them. So, I chose to keep the latitude and longitude columns since they are numeric and recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f92bc-def2-4bc6-ae76-d483e40cccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select desired rows\n",
    "#Trip Start Timestamp, Trip End Timestamp, Trip Seconds, Trip Miles, Fare, Extras\n",
    "#Company, Pickup Centroid Latitude, Pickup Centroid Longitude, Dropoff Centroid Latitude, Dropoff Centroid Longitude \n",
    "\n",
    "cab_fare_df = initial_data.iloc[: ,[2,3,4,5,10,13,16,17,18,20,21]].copy()\n",
    "print('Columns in new cab fare dataframe')\n",
    "print(cab_fare_df.columns)\n",
    "print(cab_fare_df.dtypes)\n",
    "initial_rows = len(cab_fare_df)\n",
    "print(f\"Initial length of dataframe: {initial_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab8fb1-ab5e-4c55-93ef-084ec42ab01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "cab_fare_df.drop_duplicates()\n",
    "\n",
    "print(f\"Length of dataframe after dropping duplicates: {len(cab_fare_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61490801-42cd-444b-946e-6904a36ddbbd",
   "metadata": {},
   "source": [
    "The length of the dataset did not change after dropping duplicates; all rows were unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c7212-24d7-456b-85ef-1a205ee9d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check # of NAs\n",
    "\n",
    "print(\"Columns containing null values\")\n",
    "print(cab_fare_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dacc54-b27b-476a-86f0-b28de35e0ce5",
   "metadata": {},
   "source": [
    "The latitude and longitude fields have the most null values. I do not want to simply drop them, as it's tens of thousands of rows. Instead, I'll fill the nulls with the mean for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea9f4a-e350-43c6-85cc-726540f5e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle blank latitudes and longitudes by filling in with mean\n",
    " \n",
    "cab_fare_df['Pickup Centroid Latitude'] = cab_fare_df['Pickup Centroid Latitude'].fillna(cab_fare_df['Pickup Centroid Latitude'].mean())\n",
    "\n",
    "cab_fare_df['Pickup Centroid Longitude'] = cab_fare_df['Pickup Centroid Longitude'].fillna(cab_fare_df['Pickup Centroid Longitude'].mean())\n",
    "cab_fare_df['Dropoff Centroid Latitude'] = cab_fare_df['Dropoff Centroid Latitude'].fillna(cab_fare_df['Dropoff Centroid Latitude'].mean())\n",
    "cab_fare_df['Dropoff Centroid Longitude'] = cab_fare_df['Dropoff Centroid Longitude'].fillna(cab_fare_df['Dropoff Centroid Longitude'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e71f2-1968-4496-8dff-5549d9d401e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with NA values\n",
    "print('Columns still containing null values')\n",
    "print(cab_fare_df.isnull().sum())\n",
    "cab_fare_df = cab_fare_df.dropna()\n",
    "print(f\"Length of dataframe after dropping NAs: {len(cab_fare_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa46499-c6e4-4230-9a9c-a1275615e319",
   "metadata": {},
   "source": [
    "There were much fewer null values remaining after filling in the latitudes and longitudes. I dropped the rows with remaining null values, only a total of 2,143 rows or 0.25% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c07fa1-57b0-46d4-b17e-95a16ec190ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chicago coordinates: 41.8832° N, 87.6324° W\n",
    "print(cab_fare_df['Pickup Centroid Latitude'].describe())\n",
    "print(cab_fare_df['Pickup Centroid Longitude'].describe())\n",
    "print(cab_fare_df['Dropoff Centroid Latitude'].describe())\n",
    "print(cab_fare_df['Dropoff Centroid Longitude'].describe())\n",
    "#min and max of all coordinates are appropriate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7727860-e1cf-4cc2-b7f1-ca84dcaf3cd9",
   "metadata": {},
   "source": [
    "All of the dropoff and pickup coordinates are in a reasonable range. There is no need to drop any coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e8a54-13f9-4acd-9761-ad626c3d1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view trip seconds, miles, fare, and extras\n",
    "\n",
    "print(cab_fare_df['Trip Seconds'].describe())\n",
    "print(cab_fare_df['Trip Miles'].describe())\n",
    "print(cab_fare_df['Fare'].describe())\n",
    "print(cab_fare_df['Extras'].describe())\n",
    "print(f\"Current length of dataframe: {len(cab_fare_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2511ff-b78b-4e5f-b4a7-02ee7c259664",
   "metadata": {},
   "source": [
    "For the numeric columns seconds, miles, and fare, all of the minimum values are zero. These are likely erroneous entries and should not be included in the data. I will filter out any rows where the trip is less than 60 seconds, less than zero miles, or less than one dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48dd62-d091-41b4-b3f0-d1cdac25fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter erroneous entries for miles, seconds, fare\n",
    "\n",
    "cab_fare_df = cab_fare_df[(cab_fare_df['Trip Seconds']>60) & (cab_fare_df['Trip Miles']>0) & (cab_fare_df['Fare']>1)]\n",
    "print(f\"Length of dataframe after removing erroneous entries: {len(cab_fare_df)}\")\n",
    "\n",
    "print(cab_fare_df['Trip Seconds'].describe())\n",
    "print(cab_fare_df['Trip Miles'].describe())\n",
    "print(cab_fare_df['Fare'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef076fdd-0432-403b-9e19-fffa88d39240",
   "metadata": {},
   "source": [
    "A large amount of the data was lost due to being erroneous. The number of rows fell from 863,104 to 767,127, a difference of 95,977 or 11%. \n",
    "\n",
    "Next, there is filtering out any outliers that might skew the data. I use the quantile method to define \"outlier\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a29faf-118f-4b03-bf32-02fa02209afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter outliers for seconds and miles \n",
    "\n",
    "#define a function to return the low and high thresholds for outliers\n",
    "def find_outliers(df_column):\n",
    "    q1 = df_column.quantile(0.25)\n",
    "    q3 = df_column.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    low = float(q1 - (1.5*iqr))\n",
    "    high = float(q3 + (1.5*iqr))\n",
    "    return low, high\n",
    "\n",
    "#define a function to find the percentage of values in a column that are outliers\n",
    "def find_percent_outliers(df_column):\n",
    "    low, high = find_outliers(df_column)\n",
    "    prop_outliers = 1-(df_column.between(low, high).sum()/len(df_column))\n",
    "    percent_outlier = prop_outliers*100\n",
    "    return percent_outlier\n",
    "\n",
    "print(f\"Percent of Trip Seconds that are outliers: {round(find_percent_outliers(cab_fare_df['Trip Seconds']),4)}%\")\n",
    "print(f\"Percent of Trip Miles that are outliers: {round(find_percent_outliers(cab_fare_df['Trip Miles']),4)}%\")\n",
    "\n",
    "#since % of outliers is so low, we can filter out outliers for seconds and miles\n",
    "s_low, s_high = find_outliers(cab_fare_df['Trip Seconds'])\n",
    "cab_fare_df = cab_fare_df[cab_fare_df['Trip Seconds'].between(s_low, s_high)]\n",
    "\n",
    "m_low, m_high = find_outliers(cab_fare_df['Trip Miles'])\n",
    "cab_fare_df = cab_fare_df[cab_fare_df['Trip Miles'].between(m_low, m_high)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fb173-687b-488b-a0a7-476f11693c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering outliers in fare and extras\n",
    "\n",
    "print(f\"Percent of fares that are outliers: {round(find_percent_outliers(cab_fare_df['Fare']),4)}%\")\n",
    "print(f\"Percent of extra charges that are outliers: {round(find_percent_outliers(cab_fare_df['Extras']),4)}%\")\n",
    "\n",
    "#drop these outliers\n",
    "f_low, f_high = find_outliers(cab_fare_df['Fare'])\n",
    "cab_fare_df = cab_fare_df[cab_fare_df['Fare'].between(f_low, f_high)]\n",
    "\n",
    "x_low, x_high = find_outliers(cab_fare_df['Extras'])\n",
    "cab_fare_df = cab_fare_df[cab_fare_df['Extras'].between(x_low, x_high)]\n",
    "\n",
    "print(f\"Length of dataframe after filtering outliers: {len(cab_fare_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6580f87-8bb9-4d2a-8f4b-601f9f14d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating % dropped\n",
    "cleaned_rows = len(cab_fare_df)\n",
    "dropped_rows = initial_rows - cleaned_rows\n",
    "percent_dropped = round(dropped_rows/initial_rows,4)*100\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "print(f\"Final number of rows after cleaning: {cleaned_rows}\")\n",
    "print(f\"Percent of dataset dropped: {round(percent_dropped,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df0993-8bb8-4fa4-b72b-9cb5d5b07534",
   "metadata": {},
   "source": [
    "The percentage of outliers in each column were very small, so I dropped them from the dataset to avoid skewing the data. Overall, after dealing with null values, erroneous entries and outliers, the dataset dropped 16.45% from the original, with a final row count of 722,878. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5bb01-3cbc-47f2-8c88-b6d766c89606",
   "metadata": {},
   "source": [
    "### Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c9d0e-193f-4fe3-b99d-b4c3ad71562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum fare and extras columns \n",
    "cab_fare_df['Trip Cost'] = cab_fare_df['Fare'] + cab_fare_df['Extras']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43732a40-753f-47f0-bf4f-16f337155b62",
   "metadata": {},
   "source": [
    "I summed the fare and extras columns into one \"trip cost\" column to be the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b36f4d-2ac1-4e51-a569-3ac9561d7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pickup timestamp column\n",
    "\n",
    "cab_fare_df['Trip Start Timestamp'] = pd.to_datetime(cab_fare_df['Trip Start Timestamp'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "#cab_fare_df['Trip End Timestamp'] = pd.to_datetime(cab_fare_df['Trip End Timestamp'])\n",
    "\n",
    "#create month column \n",
    "cab_fare_df['Month'] = cab_fare_df['Trip Start Timestamp'].dt.month\n",
    "print(cab_fare_df['Month'].unique())\n",
    "\n",
    "#create hour column\n",
    "cab_fare_df['Hour'] = cab_fare_df['Trip Start Timestamp'].dt.hour \n",
    "print(cab_fare_df['Hour'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8a3cd-f340-4d85-af5b-5807f3a14112",
   "metadata": {},
   "source": [
    "I converted the trip start timestamp column into month and hour columns. \n",
    "\n",
    "This dataset only contains rides from January-March 2024. The month column, therefore, will not be useful for analysis since not all the months are represented.\n",
    "\n",
    "The hours column, however, now contains the hour of the day the trip began in numeric form. All 24 hours are represented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d45866-a1ea-467b-8371-82043826608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert seconds to minutes\n",
    "cab_fare_df['Minutes'] = cab_fare_df['Trip Seconds']/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9868675e-7cb8-473b-ab7e-944a2d101a59",
   "metadata": {},
   "source": [
    "I decided to convert the seconds column into minutes to improve interpretability. It is generally easier to contextualize time in minutes than in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3289e1-e83e-443a-9f71-0962300d2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking current columns\n",
    "print(cab_fare_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f197024-fa9f-4112-9445-6feceee25e8e",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630acd59-a717-41c3-b97f-665526aa148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix for numeric variables\n",
    "correlation = cab_fare_df.corr(numeric_only=True)\n",
    "\n",
    "#heatmap of correlation matrix\n",
    "sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation for Numeric Variables in Taxi Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22846f-d3c6-4237-ab9a-39ed6d63dcd8",
   "metadata": {},
   "source": [
    "Looking at the correlation matrix heatmap, the variables most highly associated with the dependent variable, trip cost, are seconds/minutes, miles, pickup longitude. Fare and extras are associated with cost because those were the two variables summed to calculate cost. \n",
    "\n",
    "There are very weak associations between trip cost and the pickup month and hour. There is no observed association between cost and dropoff latitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9cb8e6-bd71-47e5-9c4b-833a127a0778",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2cc84-2921-4bb0-a6ef-be79bf6af771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view all taxi companies\n",
    "\n",
    "all_companies = cab_fare_df['Company'].unique()\n",
    "print(all_companies, len(all_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e0eeb-f474-46d3-a137-92fdcb41fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target encoding\n",
    "\n",
    "#sample ~20% of dataframe\n",
    "target_sample = cab_fare_df.sample(n=150000, random_state=456)\n",
    "\n",
    "target_mean = target_sample.groupby('Company')['Trip Cost'].mean()\n",
    "target_sample['Company_mean'] = target_sample['Company'].map(target_mean)\n",
    "\n",
    "company_correlation = target_sample['Company_mean'].corr(target_sample['Trip Cost'])\n",
    "print(f\"The correlation between taxi company and trip cost is {company_correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7005709-895e-49e9-9b2f-ba78373cd7f4",
   "metadata": {},
   "source": [
    "After target encoding, the correlation between company and cost was only 0.129, which is not very high. Therefore, I feel comfortable not including company as a variable in any further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a983e01-8750-4505-b740-573b78840869",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdf976-df4d-47fe-baf2-3ee4b42d68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#means of numeric columns\n",
    "\n",
    "means = cab_fare_df.mean(numeric_only=True)\n",
    "print(means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cac96e-817b-49a9-88bf-ed54480cd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sample dataframe for EDA\n",
    "\n",
    "sample = cab_fare_df.sample(n=50000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781893bd-aec5-4db1-8ea6-984be105a002",
   "metadata": {},
   "source": [
    "I created a sample dataset of 50,000 rows to make plotting more efficient and more visually appealing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c712d-b642-4b63-b0e0-fff894b2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots of seconds and miles features\n",
    " \n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Minutes\", \"Miles\"))\n",
    "\n",
    "# seconds plot\n",
    "fig.add_trace(\n",
    "    go.Box(y = sample['Minutes'], name=\"Minutes\"), row=1, col=1)\n",
    "\n",
    "# miles\n",
    "fig.add_trace(\n",
    "    go.Box(y = sample['Trip Miles'], name=\"Miles\"), row=1, col=2)\n",
    "\n",
    "fig.update_layout(width=800, height=400, title=\"Plots of Minutes and Miles\") \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0bf0a-f6f4-4f11-9723-0a47afd32243",
   "metadata": {},
   "source": [
    "The boxplot of trip time in minutes shows that the median time for trips is about 15 minutes. With the exception of outliers, the longest trip is about 55 minutes and the shortest trip is one minute. \n",
    "\n",
    "The median number of miles for a trip is 3.8 with the majority of trips being less than 28 miles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d476cbc-3b7d-48aa-a2da-ba0efb674933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograms of fares, extra charges, and total cost\n",
    "\n",
    "dist = make_subplots(rows=3, cols=1, subplot_titles=(\"Fare\", \"Extras\", \"Total Cost\"))\n",
    "\n",
    "dist.add_trace(\n",
    "    go.Histogram(x=sample[\"Fare\"], name=\"Distribution of Cab Fares\", histnorm=\"percent\",\n",
    "                xbins=dict(start=0, end=70, size=2)), row=1, col=1)\n",
    "\n",
    "dist.add_trace(\n",
    "    go.Histogram(x=sample[\"Extras\"], name=\"Distribution of Extra Charges\", histnorm=\"percent\",\n",
    "                xbins=dict(start=0, end=10, size=0.5)), row=2, col=1)\n",
    "\n",
    "dist.add_trace(\n",
    "    go.Histogram(x=sample[\"Trip Cost\"], name=\"Distribution of Total Costs\", histnorm=\"percent\",\n",
    "                xbins=dict(start=0, end=80, size=2)), row=3, col=1)\n",
    "\n",
    "dist.update_layout(width=1100, height=800, title=\"Distributions of Fares, Extras, and Totals\") \n",
    "dist.update_yaxes(title_text=\"Percentage\", row=1, col=1)\n",
    "dist.update_yaxes(title_text=\"Percentage\", row=2, col=1)\n",
    "dist.update_yaxes(title_text=\"Percentage\", row=3, col=1)\n",
    "\n",
    "dist.update_xaxes(title_text=\"Fare in USD\", row=1, col=1)\n",
    "dist.update_xaxes(title_text=\"Extra Charges in USD\", row=1, col=2)\n",
    "dist.update_xaxes(title_text=\"Total Cost in USD\", row=1, col=3)\n",
    "\n",
    "dist.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea209941-877f-4a3e-840e-814a44fb1969",
   "metadata": {},
   "source": [
    "The distributions of fare and total cost line up almost perfectly. This makes sense as total cost is the sum of fare and extras, and most extra charges fall between zero and one dollar. So cost is going to be more determined by fare than by extra charges. \n",
    "\n",
    "All three of the cost-related distributions are right-skewed. The majority of fares are cheaper but there are a few trips with higher charges that skew the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac3d15-f698-4b98-8bb3-9f85d78b1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of miles\n",
    "miles_dist = px.histogram(sample['Trip Miles'], histnorm=\"percent\", x=\"Trip Miles\")\n",
    "miles_dist.update_xaxes(title_text=\"Miles\")\n",
    "miles_dist.update_yaxes(title_text=\"Percentage\")\n",
    "miles_dist.update_layout(title=\"Distribuition of Trip Miles\")\n",
    "\n",
    "miles_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11827c3-c4e6-4ca2-8eeb-db8f0e560fb8",
   "metadata": {},
   "source": [
    "The distribution of miles is right-skewed; the mean number of miles is 6.7 while the median is 3.8. Trips going less than five miles are much more common than not. There are relatively few trips over 20 miles that skew the distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305725a-4848-4115-89a5-ce23c2eb27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of minutes\n",
    "m_dist = px.histogram(sample['Minutes'], histnorm=\"percent\", x=\"Minutes\")\n",
    "m_dist.update_traces(xbins=dict(start=0, end=60, size=1))\n",
    "m_dist.update_xaxes(title_text=\"Minutes\")\n",
    "m_dist.update_yaxes(title_text=\"Percentage\")\n",
    "m_dist.update_layout(title=\"Distribuition of Trip Minutes\")\n",
    "\n",
    "m_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2133f-f1f8-4b69-8b6e-72e208001faf",
   "metadata": {},
   "source": [
    "The distribution of trip duration in minutes is also right-skewed; the mean number of minutes is 18.48 and the median is 15.58. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e23148-ac4d-455b-9e9a-d55eba9aff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplots of miles v cost\n",
    "\n",
    "m_plot = px.scatter(sample, x=sample['Trip Miles'], y=sample['Trip Cost'], trendline='ols', trendline_color_override = 'black',\n",
    "                    render_mode=\"webgl\")\n",
    "m_plot.update_layout(title='Trip Miles vs Trip Cost in USD')\n",
    "m_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0b67a-1499-48b9-bd20-b70242ed7730",
   "metadata": {},
   "source": [
    "This plot shows that there is a positive trend between the lengths of trips and their costs; as the number of miles increases, the cost of the trip also increases. There are also a few vertical outliers closer to the lower end of the number of miles. These shorter trips are more expensive than trips of a similar distance tend to be. This could be due to extra charges or traffic increasing the duration of trip time without increasing the mileage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297f9d5-b4a2-4ec0-b2e5-8c529f6784ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot of seconds v cost\n",
    "\n",
    "s_plot = px.scatter(sample, x=sample['Minutes'], y=sample['Trip Cost'], trendline='ols', trendline_color_override = 'black',\n",
    "                    render_mode=\"webgl\")\n",
    "s_plot.update_layout(title='Trip Time in Minutes vs Trip Cost in USD')\n",
    "s_plot.update_traces(marker=dict(color='green'))\n",
    "s_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef239aa1-9b9d-4c50-a2e0-0d8c3203cecb",
   "metadata": {},
   "source": [
    "This plot also shows a positive trend between trip cost and trip time in minutes. There is more scatter in this plot than in the plot of miles. There tends to be a wider range of costs associated with any given trip duration in minutes. This could be due to differences in pricing at different hours or maybe charges associated with certain locations like airports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b08a6-2fe4-49e9-9eef-4627547152b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickup density heatmap \n",
    "\n",
    "hb = plt.hexbin(sample['Pickup Centroid Longitude'], sample['Pickup Centroid Latitude'], gridsize=50, cmap='viridis')\n",
    "plt.close()\n",
    "\n",
    "verts = hb.get_offsets()\n",
    "counts = hb.get_array()\n",
    "\n",
    "fig = go.Figure(go.Scatter(x=verts[:,0], y=verts[:,1], mode='markers', marker=dict(size=10, color=np.log1p(counts),\n",
    "        colorscale='Viridis', showscale=True, colorbar=dict(title=\"Log of Pickup Counts per Location\"))))\n",
    "\n",
    "fig.update_layout(width=1000, height=800, xaxis_title='Longitude', yaxis_title='Latitude', title='Pickup Density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15253c-a591-4f39-b15b-9c8790dba174",
   "metadata": {},
   "source": [
    "This pickup density grid shows that there are a few locations with higher ratios of pickups. The majority of the latitude/longitude pairs were tracking lower numbers of pickup orders. So much so, that I decided to use the log of the counts to show a little more distinction in pickup density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5668c-10d8-4744-958b-4a6c3956a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap of average duration of trip across different hours and distances\n",
    "\n",
    "miles = np.linspace(sample['Trip Miles'].min(), sample['Trip Miles'].max(), 28)\n",
    "sample['Miles_bin'] = pd.cut(sample['Trip Miles'], bins=miles)\n",
    "sample['Miles_bin_center'] = sample['Miles_bin'].apply(lambda x: x.mid)\n",
    "\n",
    "\n",
    "agg = sample.groupby(['Hour', 'Miles_bin_center'],observed=True)['Minutes'].mean().reset_index(name='Avg_Minutes')\n",
    "\n",
    "pivot = agg.pivot(index='Hour', columns='Miles_bin_center', values='Avg_Minutes')\n",
    "t_heatmap = go.Figure(go.Heatmap(z=pivot.values, x=pivot.columns, y=pivot.index, colorscale='Viridis',\n",
    "    colorbar=dict(title='Avg Trip Minutes')))\n",
    "\n",
    "t_heatmap.update_layout(title='Average Trip Minutes by Trip Miles and Hour of Day', xaxis_title='Trip Miles',\n",
    "    yaxis_title='Hour of Day', width=1000, height=600)\n",
    "\n",
    "t_heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78124fb-301d-4a05-a576-940896296c06",
   "metadata": {},
   "source": [
    "This heatmap illustrates the relationship between hour of day, distance of trip, and trip duration in minutes. It depicts logical conclusions like that shorter trips take less time than longer ones, regardless of time of day. Also, trips over 20 miles are rarer, especially in the early mornings. One noteworthy observation is that for trips between 15 and 20 miles, the average time it takes seems to increase from around 7-8am and again from around 3-6pm. This likely corresponds to an increase in morning and evening rush hour traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c77bd-5a32-4b62-9aad-8fdfe3f77e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked bar of hour vs avg cost\n",
    "\n",
    "avg_cost = cab_fare_df['Trip Cost'].mean()\n",
    "print(avg_cost)\n",
    "\n",
    "#bin cost\n",
    "sample['more_than_avg'] = np.where(sample['Trip Cost']>avg_cost, 'yes', 'no')\n",
    "\n",
    "grouped = (sample.groupby(['Hour', 'more_than_avg'], observed=True).size().reset_index(name='Number of Trips'))\n",
    "\n",
    "bar = px.bar(grouped, x='Hour', y='Number of Trips', color='more_than_avg', title=\"Hourly Trips by Avg Costs\")\n",
    "bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fbebf-4d42-453c-a1b5-c13f08ecccdb",
   "metadata": {},
   "source": [
    "This bar chart shows whether trip costs tend to fall above or below overall average cost based on time of day. From 8pm - 12am and then again from 5-6am, more trips cost more than the overall average of $22.29 than not. From 7am until 8pm, the numbers of trips increased and the ratio of more expensive to less expensive becomes more favorable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4633f-b7f0-46eb-b719-7115d52c738a",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff7235-08ef-413d-bb84-5eff43144302",
   "metadata": {},
   "source": [
    "The exploratory data analysis revealed that several numeric features of the dataframe are skewed. I will apply log transformation to those features to try and mitigate the skewness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb29d72-5086-41e7-97a5-e8efd9ece00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform minutes, miles, fare, extras, cost columns\n",
    "\n",
    "transform_columns = ['Minutes', 'Trip Miles', 'Trip Cost']\n",
    "cab_fare_df[transform_columns] = np.log1p(cab_fare_df[transform_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761e679-3a9d-43d8-b610-e3ef8dea62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only numeric features\n",
    "\n",
    "cab_fare_df = cab_fare_df.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1f45d-c678-470f-a7c1-d041a8d15b4e",
   "metadata": {},
   "source": [
    "### Splitting and Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b42d74-9ff4-420f-b1e6-b7818b05d69b",
   "metadata": {},
   "source": [
    "Before feature selection, I will need to split the data into testing and training sets and also apply scaling.\n",
    "\n",
    "For my features, I dropped the target variable, cost, as well as fare and extra since they are represented in the cost column. I also dropped the seconds column since I'd rather use the minutes column.\n",
    "\n",
    "I split the data 70/30 for testing and training. I then scaled the features using the StandardScaler, since outliers were already filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b899c41-207c-4fff-80cd-466d41bbfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "\n",
    "#print(cab_fare_df.columns)\n",
    "\n",
    "x = cab_fare_df.drop(['Trip Cost', 'Fare', 'Extras', 'Trip Seconds'], axis=1)\n",
    "y = cab_fare_df['Trip Cost']\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f1137-c9ee-4b4c-9712-043ea71ddd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_temp = scaler.transform(x_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dda0a-baf4-472b-abc2-c14cf101fe93",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "I used scikit-learn's Univariate Feature Selector to select the top five features for my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac83fdd-0c00-4a9a-a20e-d7df3484f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection: f_regression\n",
    "\n",
    "selector = SelectKBest(f_regression, k=5)\n",
    "selector.fit(x_train, y_train)\n",
    "\n",
    "print(\"Input features: \", selector.feature_names_in_)\n",
    "print(\"Selected Features: \", selector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e851abe-3427-4474-9d2e-c687a30898a1",
   "metadata": {},
   "source": [
    "__The top five features are: number of miles, pickup latitude and longitude, dropoff longitude, and time duration in minutes.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac4328-e7cd-4071-9694-7b586b0237d2",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe691a-57c2-4b1a-98db-2b7ab9ce2b6f",
   "metadata": {},
   "source": [
    "I split the original test set 50/50 to create a validation set. I used this validation set to test my original models and fine-tune to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33451115-cc07-49e8-a46a-dc7c23c2fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating validation set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=123)\n",
    "print(\"Records in validation set = \", len(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82135a91-195f-46a2-bd38-81b71b25159f",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "My first model was a multiple linear regression model using the five selected features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0ec90-17f6-42ea-8d02-29cb152ae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit MLR model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa302e-1c2d-4e17-9476-bcf107ce6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mlr model\n",
    "\n",
    "mlr_pred = lr.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c541fd6-f5e6-4a56-89ee-36508ef88dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undo log\n",
    "\n",
    "mlr_pred = np.expm1(mlr_pred)      \n",
    "y_val_ex = np.expm1(y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abbaec-72b0-4d6e-95c8-f23f141223ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model using mae\n",
    "\n",
    "mae_mlr = mean_absolute_error(y_val_ex, mlr_pred)\n",
    "print(\"MAE for multiple regression model: \", mae_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78676b-6256-4312-b321-a648e50970f4",
   "metadata": {},
   "source": [
    "After fitting and testing the MLR model, I had to take the exponent of the prediction results and the actual y-values to return them to their original units. \n",
    "\n",
    "I chose to use mean absolute error as the evaluation metric due to its interpretability; it is straightforward to understand that the mean error is about $3.50. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50a80d-772e-4dc7-8e83-8bbcddcf5583",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "\n",
    "I decided to build a second model, a Random Forest model, to better capture any complex, non-linear patterns in the data that the linear regression model couldn't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c30ae-a27f-469f-a926-a085a462c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=8, max_features='sqrt', bootstrap=True,random_state=123)\n",
    "rf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5b207-e52a-47a3-b8ee-d8b998125786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test random forest model\n",
    "\n",
    "rf_pred = rf.predict(x_val)\n",
    "\n",
    "rf_pred = np.expm1(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f140214-daa1-42f5-9ac6-47671a3f38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model using mae\n",
    "\n",
    "mae_rf = mean_absolute_error(y_val_ex, rf_pred)\n",
    "print(\"MAE for random forest model: \", mae_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e00042-9043-49f5-ab00-4cf279a5418b",
   "metadata": {},
   "source": [
    "I started with a smaller number of trees and a smaller max depth as a baseline for the model. When the number of trees is 100 and the max depth is 8, the MAE is 1.77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdde51-d975-4335-b311-a8e7bbc4b6d5",
   "metadata": {},
   "source": [
    " ### Fine-tuning Random Forest\n",
    "\n",
    " I changed the values of the hyperparameters from the original Random Forest model by increasing the number of trees and the max depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338110d7-c1c7-421d-b602-3bf8be81b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning random forest\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators=300, max_depth=15, max_features='sqrt', bootstrap=True,random_state=123)\n",
    "rf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c62dc-257a-4b72-9a59-662f421d93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test random forest model\n",
    "\n",
    "rf2_pred = rf2.predict(x_val)\n",
    "rf2_pred = np.expm1(rf2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e04798-3669-49d1-afac-090ef17433fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model using mae\n",
    "\n",
    "mae_rf2 = mean_absolute_error(y_val_ex, rf2_pred)\n",
    "print(\"MAE for fine-tuned random forest model: \", mae_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e184c7-c4aa-467c-9469-79c9c45d9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 score\n",
    "\n",
    "r2_rf2 = r2_score(y_val_ex, rf2_pred)\n",
    "print(\"R2 score for fine-tuned model: \", r2_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2655978-546c-4b37-9ad8-6e1488feffa1",
   "metadata": {},
   "source": [
    "I adjusted the hyperparameters several times relative to the first random forest model. Each adjustment lead to a lower and lower mean error until the final model. When the number of trees is 300 and the max depth is 15, the mean absolute error came out to 1.24. This was a satisfactory MAE but I also found the R-squared value to be sure the model was accurate. The R-squared was 0.96, which is a very high R-squared value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dd8c1-b35d-413d-8ab2-43ac0cdd74ac",
   "metadata": {},
   "source": [
    "# Final Model Test\n",
    "\n",
    "After fine-tuning and settling on the final model, I tested it on the actual test set and evaluated again using MAE and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70482e2a-0813-4afd-9bb4-0879ddc5d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undo log transformation\n",
    "\n",
    "y_test = np.expm1(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907e6e1-284c-4281-85d1-9b553eda89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on actual test set\n",
    "\n",
    "rf2_test_pred = rf2.predict(x_test)\n",
    "rf2_test_pred = np.expm1(rf2_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7d8b9-4854-4f5c-a982-06aa1df43ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "\n",
    "final_mae = mean_absolute_error(y_test, rf2_test_pred)\n",
    "final_r2 = r2_score(y_test, rf2_test_pred)\n",
    "\n",
    "print(\"Final MAE: \", final_mae)\n",
    "print(\"Final R2 score: \", final_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e05e9f-3a2a-47f7-9d2a-098aa7b8da4d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The dataset began with 23 initial features. After feature engineering, I chose the top five features to use in the model: number of miles, pickup latitude and longitude, dropoff longitude, and number of minutes. \n",
    "\n",
    "My initial model was a linear regression model with those five features. That model had a mean error of $3.50.\n",
    "\n",
    "The first random forest regressor did much better than the linear regression model, with a mean error of $1.77. \n",
    "\n",
    "After fine tuning the random forest model by increasing the number of trees and maximum depth, the mean error again decreased to $1.24, which is an acceptable mean error. I also found the R-squared value for the model as 0.96. This means 96% of the variability in the cost can be explained by the model, which is very high.\n",
    "\n",
    "I did not want to increase the complexity of the model anymore with additional trees or a higher max depth, so I chose this as the final model.\n",
    "\n",
    "When testing the final model on the test set, the mean error was the same as the validation set's, $1.24 after rounding. The R-squared value also remained the same, 0.96. This model was able to predict trip cost with an average error of only a little over one dollar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131d024-8551-4dc0-8d3d-2c9b4099e477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
